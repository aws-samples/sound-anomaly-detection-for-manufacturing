{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound anomaly detection\n",
    "*Step 3 - Performing anomaly detection with a computer vision based approach, leveraging Amazon Rekognition Custom Labels*\n",
    "\n",
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the spectrograms directly as inputs to feed a computer vision-based architecture. We will leverage Amazon Rekognition Custom Labels. Training a custom label project follows this process:\n",
    "1. Building the datasets and uploading them to Amazon S3\n",
    "2. Creating a project and collecting the generated project ARN\n",
    "3. Associate the project with the training data, validation data and output locations\n",
    "4. Train a project version with these datasets\n",
    "5. Start the model: this will provision an endpoint and deploy the model behind it\n",
    "6. Query the endpoint for inference for the validation and testing datasets\n",
    "\n",
    "You need to ensure that this **notebook instance has an IAM role** which allows it to call the **Amazon Rekognition Custom Labels API**:\n",
    "1. In your IAM console, look for the SageMaker execution role endorsed by your notebook instance (a role with a name like *AmazonSageMaker-ExecutionRole-yyyymmddTHHMMSS*)\n",
    "2. Click on **Attach Policies** and look for this managed policy: **AmazonRekognitionCustomLabelsFullAccess**\n",
    "3. Check the box next to it and click on **Attach Policy**\n",
    "\n",
    "Your SageMaker notebook instance can now call the Rekognition Custom Labels APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Other imports:\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# AWS libraries:\n",
    "import boto3\n",
    "\n",
    "sys.path.append('tools')\n",
    "import sound_tools\n",
    "import utils\n",
    "import rekognition_tools as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization:\n",
    "%matplotlib inline\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "plt.style.use('Solarize_Light2')\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "# Paths definition:\n",
    "DATA           = os.path.join('data', 'interim')\n",
    "RAW_DATA       = os.path.join('data', 'raw')\n",
    "PROCESSED_DATA = os.path.join('data', 'processed')\n",
    "TRAIN_PATH     = os.path.join(PROCESSED_DATA, 'train')\n",
    "TEST_PATH      = os.path.join(PROCESSED_DATA, 'test')\n",
    "\n",
    "os.makedirs(os.path.join(PROCESSED_DATA, 'train', 'normal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DATA, 'train', 'abnormal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DATA, 'test', 'normal'), exist_ok=True)\n",
    "os.makedirs(os.path.join(PROCESSED_DATA, 'test', 'abnormal'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering parameters\n",
    "These parameters are used to extract features from sound files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 64\n",
    "frames = 5\n",
    "n_fft = 1024\n",
    "hop_length = 512\n",
    "power = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1:** Building the datasets\n",
    "---\n",
    "### Generate list of sound files and splitting them\n",
    "We are going to generate a spectrogram for each signal and use this as input to train a custom labels model with Rekognition:\n",
    "\n",
    "* Testing dataset: **1110 signals** including:\n",
    "  * 295 abnormal signals\n",
    "  * 815 normal signals\n",
    "* Training dataset: **4390 signals** including:\n",
    "  * 1180 abnormal signals\n",
    "  * 3210 normal signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the list of normal and abnormal files:\n",
    "normal_files, abnormal_files = utils.build_files_list(root_dir=os.path.join(DATA, 'fan'))\n",
    "\n",
    "# Concatenate them to obtain a features and label datasets that we can split:\n",
    "X = np.concatenate((normal_files, abnormal_files), axis=0)\n",
    "y = np.concatenate((np.zeros(len(normal_files)), np.ones(len(abnormal_files))), axis=0)\n",
    "\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(X, y,\n",
    "                                                                      train_size=0.8,\n",
    "                                                                      random_state=42,\n",
    "                                                                      shuffle=True,\n",
    "                                                                      stratify=y\n",
    "                                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating spectrograms pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_files = sound_tools.generate_spectrograms(train_files, os.path.join(PROCESSED_DATA, 'train'))\n",
    "img_test_files = sound_tools.generate_spectrograms(test_files, os.path.join(PROCESSED_DATA, 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 buckets preparation\n",
    "We upload the train and test dataset to S3 and generate the manifest files. **Update the BUCKET variable with your own Bucket name below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = '<YOUR-BUCKET-NAME>'\n",
    "PREFIX = 'custom-label'\n",
    "LABELS = ['abnormal', 'normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $TRAIN_PATH s3://$BUCKET/$PREFIX/train\n",
    "!aws s3 cp --recursive $TEST_PATH s3://$BUCKET/$PREFIX/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.create_manifest_from_bucket(BUCKET, PREFIX, 'train', LABELS, output_bucket=f's3://{BUCKET}/{PREFIX}/manifests')\n",
    "rt.create_manifest_from_bucket(BUCKET, PREFIX, 'test', LABELS, output_bucket=f's3://{BUCKET}/{PREFIX}/manifests')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2:** Creating a custom label project in Amazon Rekognition\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization, get a Rekognition client:\n",
    "PROJECT_NAME = 'sound-anomaly-detection'\n",
    "reko = boto3.client(\"rekognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to create a Rekognition project:\n",
    "try:\n",
    "    project_arn = reko.create_project(ProjectName=PROJECT_NAME)['ProjectArn']\n",
    "    \n",
    "# If the project already exists, we get its ARN:\n",
    "except reko.exceptions.ResourceInUseException:\n",
    "    # List all the existing project:\n",
    "    print('Project already exists, collecting the ARN.')\n",
    "    reko_project_list = reko.describe_projects()\n",
    "    \n",
    "    # Loop through all the Rekognition projects:\n",
    "    for project in reko_project_list['ProjectDescriptions']:\n",
    "        # Get the project name (the string after the first delimiter in the ARN)\n",
    "        project_name = project['ProjectArn'].split('/')[1]\n",
    "        \n",
    "        # Once we find it, we store the ARN and break out of the loop:\n",
    "        if (project_name == PROJECT_NAME):\n",
    "            project_arn = project['ProjectArn']\n",
    "            break\n",
    "            \n",
    "project_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3:** Associate the dataset to the project\n",
    "---\n",
    "We need to tell Rekognition where to find the training data, testing data and where to output its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingData = {\n",
    "    'Assets': [{ \n",
    "        'GroundTruthManifest': {\n",
    "            'S3Object': { \n",
    "                'Bucket': BUCKET,\n",
    "                'Name': f'{PREFIX}/manifests/train.manifest'\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    "}\n",
    "\n",
    "TestingData = {\n",
    "    'AutoCreate': True\n",
    "}\n",
    "\n",
    "OutputConfig = { \n",
    "    'S3Bucket': BUCKET,\n",
    "    'S3KeyPrefix': f'{PREFIX}/output'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4:** Now we create a project version\n",
    "---\n",
    "Creating a project version will build and train a model within this Rekognition project for the data previously configured. Project creation can fail, if the bucket you selected cannot be accessed by Rekognition. Make sure the following Bucket Policy is applied to your bucket (replace **<YOUR-BUCKET-NAME>** by your bucket):\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AWSRekognitionS3AclBucketRead20191011\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"rekognition.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetBucketAcl\",\n",
    "                \"s3:GetBucketLocation\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::<YOUR-BUCKET-NAME>\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"AWSRekognitionS3GetBucket20191011\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"rekognition.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetObjectAcl\",\n",
    "                \"s3:GetObjectVersion\",\n",
    "                \"s3:GetObjectTagging\"\n",
    "            ],\n",
    "            \"Resource\": \"arn:aws:s3:::<YOUR-BUCKET-NAME>/*\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"AWSRekognitionS3ACLBucketWrite20191011\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"rekognition.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"s3:GetBucketAcl\",\n",
    "            \"Resource\": \"arn:aws:s3:::<YOUR-BUCKET-NAME>\"\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"AWSRekognitionS3PutObject20191011\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"rekognition.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"s3:PutObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::<YOUR-BUCKET-NAME>/*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"s3:x-amz-acl\": \"bucket-owner-full-control\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'experiment-1'\n",
    "VERSION_NAME = f'{PROJECT_NAME}.{version}'\n",
    "\n",
    "# Let's try to create a new project version in the current project:\n",
    "try:\n",
    "    project_version_arn = reko.create_project_version(\n",
    "        ProjectArn=project_arn,      # Project ARN\n",
    "        VersionName=VERSION_NAME,    # Name of this version\n",
    "        OutputConfig=OutputConfig,   # S3 location for the output artefact\n",
    "        TrainingData=TrainingData,   # S3 location of the manifest describing the training data\n",
    "        TestingData=TestingData      # S3 location of the manifest describing the validation data\n",
    "    )['ProjectVersionArn']\n",
    "    \n",
    "# If a project version with this name already exists, we get its ARN:\n",
    "except reko.exceptions.ResourceInUseException:\n",
    "    # List all the project versions (=models) for this project:\n",
    "    print('Project version already exists, collecting the ARN:', end=' ')\n",
    "    reko_project_versions_list = reko.describe_project_versions(ProjectArn=project_arn)\n",
    "    \n",
    "    # Loops through them:\n",
    "    for project_version in reko_project_versions_list['ProjectVersionDescriptions']:\n",
    "        # Get the project version name (the string after the third delimiter in the ARN)\n",
    "        project_version_name = project_version['ProjectVersionArn'].split('/')[3]\n",
    "\n",
    "        # Once we find it, we store the ARN and break out of the loop:\n",
    "        if (project_version_name == VERSION_NAME):\n",
    "            project_version_arn = project_version['ProjectVersionArn']\n",
    "            break\n",
    "            \n",
    "print(project_version_arn)\n",
    "status = reko.describe_project_versions(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[project_version_arn.split('/')[3]]\n",
    ")['ProjectVersionDescriptions'][0]['Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loops prints the project version training status (`TRAINING_IN_PROGRESS`) until the model has been trained (`TRAINING_COMPLETE`): if it's already trained the model status will either be:\n",
    "* `STOPPED`: the model is trained, but is not currently deployed\n",
    "* `STARTED`: the model has been deployed behind an endpoint and is available to deliver inference (hourly costs are incurred)\n",
    "* `STARTING`: deployment in progress\n",
    "* `STOPPING`: stopping in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loops while training of this project version is in progress:\n",
    "while status == 'TRAINING_IN_PROGRESS':\n",
    "    status = reko.describe_project_versions(\n",
    "        ProjectArn=project_arn,\n",
    "        VersionNames=[project_version_arn.split('/')[3]]\n",
    "    )['ProjectVersionDescriptions'][0]['Status']\n",
    "\n",
    "    print(status)\n",
    "    time.sleep(60)\n",
    "    \n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5:** Model starting\n",
    "We now have a trained model, we need to start it to serve inferences: the following command put the model in a \"hosted\" state. This process takes a while, as in the background we are created a dedicated endpoint on which we will deploy our trained model to serve the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.start_model(project_arn, project_version_arn, VERSION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6:** Model evaluation\n",
    "---\n",
    "We now have a live endpoint with our model ready to deliver its predictions.\n",
    "### Apply model on a test dataset\n",
    "Let's now get the predictions on the **test datasets**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "test_results_filename = os.path.join(PROCESSED_DATA, f'results_rekognition_{PROJECT_NAME}-{version}.csv')\n",
    "print(f'Looking for test results file: \"{test_results_filename}\"')\n",
    "\n",
    "if os.path.exists(test_results_filename):\n",
    "    print('Prediction file on the test dataset exists, loading them from disk')\n",
    "    test_predictions = pd.read_csv(test_results_filename)\n",
    "    \n",
    "else:\n",
    "    print('Predictions file on the test dataset does not exist, querying the endpoint to collect inference results...')\n",
    "    predictions_ok = rt.get_results(project_version_arn, BUCKET, s3_path=f'{BUCKET}/{PREFIX}/test/normal', label='normal', verbose=True)\n",
    "    predictions_ko = rt.get_results(project_version_arn, BUCKET, s3_path=f'{BUCKET}/{PREFIX}/test/abnormal', label='abnormal', verbose=True)\n",
    "\n",
    "    print('\\nWriting predictions for test set to disk.')\n",
    "    test_predictions = pd.concat([predictions_ok, predictions_ko], axis='index')\n",
    "    test_predictions = rt.reshape_results(test_predictions)\n",
    "    test_predictions.to_csv(test_results_filename, index=None)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.generate_error_types(test_predictions, normal_label='normal', anomaly_label='abnormal')\n",
    "tp = df['TP'].sum()\n",
    "tn = df['TN'].sum()\n",
    "fn = df['FN'].sum()\n",
    "fp = df['FP'].sum()\n",
    "\n",
    "utils.print_confusion_matrix(confusion_matrix(test_predictions['Ground Truth'], test_predictions['Prediction']), class_names=['abnormal', 'normal']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"\"\"Amazon Rekognition custom model metrics:\n",
    "- Precision: {precision*100:.1f}%\n",
    "- Recall: {recall*100:.1f}%\n",
    "- Accuracy: {accuracy*100:.1f}%\n",
    "- F1 Score: {f1_score*100:.1f}%\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "We need to stop the running model as we will continue to incur costs while the endpoint is live:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt.stop_model(project_version_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
